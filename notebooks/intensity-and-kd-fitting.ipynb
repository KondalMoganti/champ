{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intensity and $K_D$ Analysis\n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################\n",
    "#                                                                                  #\n",
    "# Leave these values blank to let the script determine them automatically.         #\n",
    "# Only set them if something goes wrong!                                           #\n",
    "#                                                                                  #\n",
    "####################################################################################\n",
    "flow_cell_id = ''\n",
    "target_name = ''\n",
    "neg_control_target_name = ''\n",
    "all_channels = []\n",
    "data_channel = ''\n",
    "target_sequence_file = \"/shared/targets.yml\"\n",
    "#nonneg_lda_weights_fpath = '/shared/yeast_beast_LDA_weights.txt'  # for microscope 3\n",
    "nonneg_lda_weights_fpath = '/shared/bLDA_coef_nonneg.txt'  # for microscope 2 and 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import glob\n",
    "import h5py\n",
    "import itertools\n",
    "import lomp\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pkg_resources\n",
    "import progressbar\n",
    "import random\n",
    "import re\n",
    "from scipy import stats\n",
    "import sys\n",
    "import yaml\n",
    "from champ import misc, intensity, initialize, seqtools, gbtools, interactive\n",
    "from champ.kd import fit_all_kds, saturated_at_concentration\n",
    "\n",
    "try:\n",
    "    matplotlib.style.use('flab')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_cell_id = interactive.determine_flow_cell_id(flow_cell_id)\n",
    "target_name = interactive.load_config_value('perfect_target_name', target_name)\n",
    "neg_control_target_name = interactive.load_config_value('neg_control_target_name', neg_control_target_name)\n",
    "all_channels = list(map(str, initialize.determine_channel_names('.'))) if not all_channels else all_channels\n",
    "alignment_channel = interactive.load_config_value('alignment_channel', data_channel)\n",
    "data_channel = interactive.determine_data_channel(all_channels, alignment_channel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_name_dir = os.path.join('/shared', flow_cell_id, 'read_names')\n",
    "bamfile_path = os.path.join('/shared', flow_cell_id, 'all_fastqs', 'genomic.bam')\n",
    "read_name_kd_filename = os.path.join('results', 'cluster-data.h5')\n",
    "read_names_by_seq_fpath = os.path.join(read_name_dir, 'read_names_by_seq.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(target_sequence_file) as f:\n",
    "    targets = yaml.load(f)\n",
    "\n",
    "target = targets[target_name]\n",
    "neg_control_target = targets[neg_control_target_name]\n",
    "\n",
    "print('Flow Cell ID: {}'.format(flow_cell_id))\n",
    "print('Target {}: {}'.format(target_name, target))\n",
    "print('Neg control target {}: {}'.format(neg_control_target_name, neg_control_target))\n",
    "print('Channels: {}'.format(all_channels))\n",
    "print('Protein channel: {}'.format(data_channel))\n",
    "print('Output file: {}'.format(read_name_kd_filename))\n",
    "print('Git commit used for this analysis: {}'.format(commit))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find all synthetic library sequences for this target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interesting_seqs = set()\n",
    "    \n",
    "stretch = set()\n",
    "for i in range(1, len(target)+1):\n",
    "    stretch.update(seqtools.get_stretch_of_complement_seqs(target, i))\n",
    "insertions = set()\n",
    "for i in range(1, 3):\n",
    "    insertions.update(seqtools.get_contiguous_insertion_seqs(target, i))\n",
    "for i in range(1, 3):\n",
    "    insertions.update(seqtools.get_insertion_seqs(target, i))   \n",
    "deletions = set()\n",
    "for i in range(1, 3):\n",
    "    deletions.update(seqtools.get_deletion_seqs(target, i))\n",
    "mismatches = set()\n",
    "for i in range(1, 3):\n",
    "    mismatches.update(seqtools.get_mismatch_seqs(target, i))\n",
    "six_n_pam = seqtools.get_randomized_pam_seqs(target, 4, 6)\n",
    "other_targets = set()\n",
    "for s in targets.values():\n",
    "    other_targets.add(s)\n",
    "\n",
    "interesting_seqs.update(other_targets)\n",
    "interesting_seqs.update(stretch)\n",
    "interesting_seqs.update(insertions)\n",
    "interesting_seqs.update(deletions)\n",
    "interesting_seqs.update(mismatches)\n",
    "interesting_seqs.update(six_n_pam)\n",
    "\n",
    "print(\"Interesting sequences: %d\" % len(interesting_seqs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Interesting Sequence Read Names File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from champ.seqtools import build_interesting_sequences\n",
    "interesting_read_names_filename = os.path.join(read_name_dir, 'interesting_{target_name}_reads_by_seq.txt'.format(target_name=target_name))\n",
    "if os.path.exists(interesting_read_names_filename):\n",
    "    # No need to recalculate, we can just load this from disk\n",
    "    interesting_read_names = {}\n",
    "    with open(interesting_read_names_filename) as f:\n",
    "        for line in f:\n",
    "            line = line.split(\"\\t\")\n",
    "            sequence = line[0]\n",
    "            read_names = line[1:]\n",
    "            interesting_read_names[sequence] = read_names\n",
    "else:\n",
    "    interesting_read_names = build_interesting_sequences(read_names_by_seq_fpath, interesting_seqs)\n",
    "    with open(interesting_read_names_filename, 'w') as f:\n",
    "        for sequence, read_names in interesting_read_names.items():\n",
    "            f.write(\"%s\\t%s\\n\" % (sequence, \"\\t\".join(read_names)))\n",
    "            \n",
    "print(\"Found read names for %d sequences of interest.\" % len(interesting_read_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Data\n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_read_name_fpath = os.path.join(read_name_dir, 'all_read_names.txt')\n",
    "target_read_name_fpath = os.path.join(read_name_dir, 'target_{}_read_names.txt'.format(target_name.lower()))\n",
    "perfect_target_read_name_fpath = os.path.join(read_name_dir, 'perfect_target_{}_read_names.txt'.format(target_name.lower()))\n",
    "neg_control_target_read_name_fpath = os.path.join(read_name_dir, 'perfect_target_{}_read_names.txt'.format(neg_control_target_name.lower()))\n",
    "phiX_read_name_fpath = os.path.join(read_name_dir, 'phix_read_names.txt')\n",
    "\n",
    "all_read_names = set(line.strip() for line in open(all_read_name_fpath))\n",
    "print(\"All read names: %d\" % len(all_read_names))\n",
    "target_read_names = set(line.strip() for line in open(target_read_name_fpath))\n",
    "print(\"Target read names: %d\" % len(target_read_names))\n",
    "perfect_target_read_names = set(line.strip() for line in open(perfect_target_read_name_fpath))\n",
    "print(\"Perfect target read names: %d\" % len(perfect_target_read_names))\n",
    "neg_control_target_read_names = set(line.strip() for line in open(neg_control_target_read_name_fpath))\n",
    "print(\"Negative control read names: %d\" % len(neg_control_target_read_names))\n",
    "phiX_read_names = set(line.strip() for line in open(phiX_read_name_fpath))\n",
    "print(\"Phix read names: %d\" % len(phiX_read_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h5_fpaths = glob.glob('*.h5')\n",
    "h5_fpaths.sort(key=misc.parse_concentration)\n",
    "for fpath in h5_fpaths:\n",
    "    print misc.parse_concentration(fpath), fpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dirs = [\n",
    "    os.path.join('results', \n",
    "                 os.path.splitext(os.path.basename(h5_fpath))[0])\n",
    "    for h5_fpath in h5_fpaths\n",
    "]\n",
    "for d in results_dirs:\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determine intensities of all aligned reads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print 'Loading data...'\n",
    "int_scores = intensity.IntensityScores(h5_fpaths)\n",
    "int_scores.get_LDA_scores(results_dirs, nonneg_lda_weights_fpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "print 'Normalizing data...'\n",
    "int_scores.normalize_scores()\n",
    "print 'Done normalizing.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_scores.plot_aligned_images('br', 'o*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_scores.plot_normalization_constants()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_scores.print_reads_per_channel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The number of observations we require to consider a cluster valid enough for fitting.\n",
    "# Clusters at the edge of a field of view might not be visible in every concentration due to\n",
    "# random imperfections in the motion of the stage, and some fields of view might simply not\n",
    "# align under each concentration.\n",
    "minimum_observations_per_cluster = len(h5_fpaths) - 3\n",
    "\n",
    "int_scores.build_good_read_names(5)\n",
    "good_read_names = int_scores.good_read_names\n",
    "good_perfect_read_names = perfect_target_read_names & good_read_names\n",
    "print 'Good Reads:', len(good_read_names)\n",
    "print 'Good Perfect Reads:', len(good_perfect_read_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_scores.build_score_given_read_name_given_channel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aligned_read_names = set()\n",
    "for h5_fpath in h5_fpaths:\n",
    "    for d in int_scores.scores[h5_fpath][data_channel].values():\n",
    "        for read_name in d.keys():\n",
    "            aligned_read_names.add(read_name)\n",
    "print('Aligned reads in data channel: {}'.format(len(aligned_read_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save read names to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_dt = h5py.special_dtype(vlen=str)\n",
    "aligned_read_names = list(sorted(aligned_read_names))\n",
    "\n",
    "with h5py.File(read_name_kd_filename, 'w') as h5:\n",
    "    h5.create_dataset('read_names', data=aligned_read_names, dtype=string_dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save sequence identifier intensities to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_name_intensities = defaultdict(list)\n",
    "for h5_fpath in h5_fpaths:\n",
    "    score_given_read_name = int_scores.score_given_read_name_in_channel[h5_fpath][data_channel]\n",
    "    for read_name in aligned_read_names:\n",
    "        intensity = score_given_read_name.get(read_name, np.nan)\n",
    "        read_name_intensities[read_name].append(intensity)\n",
    "\n",
    "intensity_matrix = []\n",
    "for read_name in aligned_read_names:\n",
    "    intensity_matrix.append(read_name_intensities[read_name])\n",
    "    \n",
    "intensity_matrix = np.array(intensity_matrix)\n",
    "\n",
    "with h5py.File(read_name_kd_filename, 'a') as h5:\n",
    "    h5.create_dataset('intensities', data=intensity_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determine what a saturated cluster looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the KD of the perfect target so we can determine at what concentrations the clusters\n",
    "# should be saturated\n",
    "sequence_read_name_intensities = defaultdict(list)\n",
    "for sequence, read_names in interesting_read_names.items():\n",
    "    for read_name in aligned_read_names:\n",
    "        if read_name not in read_name_intensities:\n",
    "            continue\n",
    "        sequence_read_name_intensities[sequence].append(read_name_intensities[read_name])\n",
    "\n",
    "all_concentrations = [misc.parse_concentration(h5_fpath) for h5_fpath in h5_fpaths]\n",
    "sequence, kd, kd_uncertainty, yint, perfect_delta_y, counts = list(fit_all_kds({target: sequence_read_name_intensities[target]}, all_concentrations, process_count=1))[0]\n",
    "print(\"Perfect target KD is %.1f +/- %.3f nM\" % (kd, kd_uncertainty))\n",
    "\n",
    "\n",
    "# Determine the median intensity of a saturated cluster\n",
    "saturated = saturated_at_concentration(kd)\n",
    "saturated_indexes = [index for index, concentration in enumerate(all_concentrations) if concentration > saturated]\n",
    "if not saturated_indexes:\n",
    "    # this should never happen, but we'll try to take something reasonable\n",
    "    print(\"Warning: perfect target sequence probably did not saturate its target!\")\n",
    "    saturated_indexes = len(all_concentrations) - 1\n",
    "    \n",
    "saturated_intensities = []\n",
    "for intensity_gradient in sequence_read_name_intensities[target]:\n",
    "    for index in saturated_indexes:\n",
    "        try:\n",
    "            value = intensity_gradient[index]\n",
    "            if not np.isnan(value):\n",
    "                saturated_intensities.append(value)\n",
    "        except IndexError:\n",
    "            continue\n",
    "median_saturated_intensity = np.median(saturated_intensities)\n",
    "print(\"Median saturated intensity: %d (N=%d)\" % (median_saturated_intensity, len(saturated_intensities)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determine the KDs of all synthetic sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_kds = {}\n",
    "with progressbar.ProgressBar(max_value=len(sequence_read_name_intensities)) as pbar:\n",
    "    for sequence, kd, kd_uncertainty, yint, delta_y, count in pbar(fit_all_kds(sequence_read_name_intensities, all_concentrations, process_count=8, delta_y=median_saturated_intensity)):\n",
    "        if count >= 5:\n",
    "            sequence_kds[sequence] = kd, kd_uncertainty, yint, delta_y, count\n",
    "print(\"Found KDs for %d of %d sequences of interest\" % (len(sequence_kds), len(sequence_read_name_intensities)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the KDs to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_dt = h5py.special_dtype(vlen=str)\n",
    "kd_dt = np.dtype([('sequence', string_dt),\n",
    "                  ('kd', np.float),\n",
    "                  ('kd_uncertainty', np.float),\n",
    "                  ('y_intercept', np.float),\n",
    "                  ('delta_y', np.float),\n",
    "                  ('count', np.int32)])\n",
    "with h5py.File(read_name_kd_filename, 'a') as h5:\n",
    "    dataset = h5.create_dataset('synthetic-kds', (len(sequence_kds),), dtype=kd_dt)\n",
    "    dataset[...] = [(sequence, kd, kd_uncertainty, yint, delta_y, count) for sequence, (kd, kd_uncertainty, yint, delta_y, count) in sequence_kds.items()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histogram_kds = [k[0] for k in sequence_kds.values()]\n",
    "histogram_counts = [k[4] for k in sequence_kds.values()]\n",
    "fig, (kd_ax, count_ax) = plt.subplots(1, 2, figsize=(16,6))\n",
    "kd_ax.hist(histogram_kds, bins=100);\n",
    "kd_ax.set_ylabel(\"Count\")\n",
    "kd_ax.set_xlabel(\"$K_D$ (nM)\")\n",
    "kd_ax.set_title(\"Histogram of Synthetic Target KDs\")\n",
    "\n",
    "count_median = np.median(histogram_counts)\n",
    "count_iqr = stats.iqr(histogram_counts)\n",
    "count_ax.hist(histogram_counts, bins=20, range=(0, int(count_median+count_iqr*5)))\n",
    "count_ax.set_ylabel(\"Count\")\n",
    "count_ax.set_xlabel(\"Clusters per Sequence\")\n",
    "count_ax.set_title(\"Representation of Synthetic Sequences\")\n",
    "fig.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
